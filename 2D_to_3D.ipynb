{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c62c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import ignite\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pytorch_ssim\n",
    "from ignite.metrics import SSIM, PSNR\n",
    "import glob\n",
    "import numpy as np\n",
    "from monai.networks.nets import UNet\n",
    "import monai\n",
    "from monai import transforms\n",
    "from monai.utils import set_determinism, first\n",
    "from monai.transforms import(\n",
    "    AsDiscrete,\n",
    "    AddChanneld,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandSpatialCropSamplesd,\n",
    "    RandShiftIntensityd,\n",
    "    RandZoomd,\n",
    "    ScaleIntensityd,\n",
    "    Spacingd,\n",
    "    SpatialPadd,\n",
    "    GaussianSmoothd,\n",
    "    RandRotate90d,\n",
    "    ToTensord,\n",
    "    RandSpatialCropd,\n",
    "    RandGaussianSmoothd,    \n",
    "    RandGaussianSharpend,\n",
    "    RandGaussianNoise,\n",
    "    \n",
    ")\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    Dataset,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from monai.inferers import sliding_window_inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd15946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feddece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.network_swinir import SwinIR as net\n",
    "\n",
    "model = monai.networks.nets.UNet(\n",
    "        spatial_dims=2,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(64, 128, 256, 512),\n",
    "        strides=(2, 2, 2),\n",
    "        num_res_units=3,\n",
    "    ).to(device)\n",
    "\n",
    "# model = net(upscale=1, in_chans=1, img_size=192, window_size=8,\n",
    "#                     img_range=1., depths=[4, 4, 4, 4], embed_dim=90, num_heads=[5, 5, 5, 5],\n",
    "#                     mlp_ratio=2, upsampler='', resi_connection='1conv').to(device)\n",
    "model.load_state_dict(torch.load(os.path.join(\"./weight/UNet_ISMRM_v3.pth\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir\n",
    "# data_dir2 = glob.glob('/nfs/hufsaims/kjh_shared/pvs_0726/EP_infant_MR_data_curated/*/nifti_3D_iso/T2_dlseg_v0221.nii.gz')\n",
    "# print(len(data_dir),len(data_dir2)) #둘다 잘 있음 굿 ㅎㅎ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ab622",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_path =  glob.glob('/nfs/hufsaims/kjh_shared/euna/ISMRM/24mo/*/3D_T2.nii.gz')\n",
    "low_path = glob.glob('/nfs/hufsaims/kjh_shared/euna/ISMRM/24mo/*/low_3d_T2.nii.gz')\n",
    "\n",
    "valid_ind = np.arange(40, 47)\n",
    "test_dict = [\n",
    "{\n",
    "    'image' : low_path[i], # low\n",
    "    'original' : origin_path[i], # original\n",
    "}for i in valid_ind\n",
    "    ] \n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "          transforms.LoadImaged(keys=[\"image\",'original']),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\",'original']),\n",
    "#         transforms.Spacingd(keys=['image'], pixdim = (1, .5, .5),mode = 'bilinear'),\n",
    "        transforms.ScaleIntensityd(\n",
    "                keys=[\"image\",'original'],\n",
    "                minv=0.0,\n",
    "                maxv=1.0,\n",
    "        ),           \n",
    "#         transforms.SpatialPadd(keys=['image'],spatial_size=(192, 192, 192)),\n",
    "#         transforms.CenterSpatialCropd(keys=['image'],roi_size = (216, 256, 256)),\n",
    "        transforms.CenterSpatialCropd(keys=['image','original'],roi_size = (192, 220, 192)),\n",
    "    ]\n",
    ")\n",
    "test_ds = CacheDataset(data=test_dict, transform=val_transforms)\n",
    "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95e10cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "improved = nib.load('/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/SwinIR/improved/syn_T2_5_6.nii.gz')\n",
    "improved = improved.get_fdata()\n",
    "\n",
    "low = nib.load('/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/SwinIR/low/low_T2_5.nii.gz')\n",
    "low = low.get_fdata()\n",
    "origin = nib.load('/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/SwinIR/origin/T2_5.nii.gz')\n",
    "origin = origin.get_fdata()\n",
    "\n",
    "unet = nib.load('/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/UNet/improved/syn_T2_5_6.nii.gz')\n",
    "unet = unet.get_fdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b77359",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.subplot(141)\n",
    "plt.imshow(origin[:,106,:],cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(142)\n",
    "plt.imshow(low[:,106,:],cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(143)\n",
    "plt.imshow(improved[:,106,:],cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.subplot(144)\n",
    "plt.imshow(unet[:,106,:],cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        images = batch['image']\n",
    "        origin = batch['original']\n",
    "        \n",
    "#         new = np.zeros((images.shape[2], images.shape[3], zmax-zmin+1))\n",
    "        print(images.shape)\n",
    "#         break\n",
    "        images = images.squeeze(1)\n",
    "        \n",
    "    \n",
    "        plt.figure(dpi = 200)\n",
    "        plt.subplot(1,4,1)\n",
    "        plt.imshow(np.rot90(images[0, :,170,:]),cmap = 'gray')\n",
    "        plt.subplot(1,4,2)\n",
    "        plt.imshow(np.rot90(images[0,:, 100,:]),cmap = 'gray')\n",
    "        plt.subplot(1,4,3)\n",
    "\n",
    "        plt.imshow(np.rot90(images[0,:,80,:]),cmap = 'gray')\n",
    "        plt.subplot(1,4,4)\n",
    "\n",
    "        plt.imshow(np.rot90(images[0,:,0,:]),cmap = 'gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_inputs.shape)\n",
    "print(val_outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4799cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb3bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b61e81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "model.eval()\n",
    "psnr_value = []\n",
    "ssim_value = []\n",
    "\n",
    "\n",
    "#bcp\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        zmin = 0\n",
    "        zmax= 219\n",
    "        \n",
    "        file_name=low_path[step].split('/')[-2]\n",
    "       \n",
    "        images = batch['image'] #1, 1, 192, 220, 192\n",
    "        origins = batch['original']#1, 1, 192, 220, 192\n",
    "        info = nib.load(batch['image_meta_dict']['filename_or_obj'][0]) #300, 320, 208\n",
    "        \n",
    "        \n",
    "        low = np.zeros((images.shape[2], zmax-zmin+1, images.shape[4]))  \n",
    "        origin = np.zeros((images.shape[2], zmax-zmin+1, images.shape[4]))\n",
    "        output = np.zeros((images.shape[2],zmax-zmin+1,images.shape[4]))\n",
    "        \n",
    "#         images = images.squeeze(0)\n",
    "#         origins = origins.squeeze(0)\n",
    "# #         break\n",
    "        for idx, i in enumerate(range(zmin, 220)):\n",
    "            val_inputs =images[...,i,:].to(device)\n",
    "            orig_crop =origins[...,i,:]\n",
    "#             val_outputs = sliding_window_inference(\n",
    "#                 val_inputs, [192, 192], 4, model, overlap=0.99,mode='gaussian')\n",
    "            val_outputs = model(val_inputs)\n",
    "            plt.figure(dpi = 200)\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(val_inputs[0,0,...].detach().cpu().numpy(),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(val_outputs[0,0,...].detach().cpu().numpy(),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "            output[:,idx,:] = val_outputs[0,0].detach().cpu().numpy()\n",
    "            origin[:,idx,:] = orig_crop[0,0]\n",
    "            low[:,idx,:] = val_inputs[0,0].detach().cpu().numpy()\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "#         low_res_nifti = nib.Nifti1Image(low,info.affine,info.header)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/SwinIR/low/low_T2_{step}.nii.gz') \n",
    "        \n",
    "#         originals = nib.Nifti1Image(origin,info.affine,info.header)\n",
    "#         nib.save(originals,f'/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/SwinIR/origin/T2_{step}.nii.gz') \n",
    "        \n",
    "        output = nib.Nifti1Image(output,info.affine,info.header)\n",
    "        nib.save(output,f'/nfs/hufsaims/kjh_shared/euna/ISMRM/Result/UNet/improved/syn_T2_{step}_6.nii.gz')  \n",
    "        \n",
    "#         low_res_nifti = nib.Nifti1Image(new1, af)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/result/0902/input_T2_{step}.nii.gz')   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15595035",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "x = 22\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        zmin = 50\n",
    "        zmax= 150\n",
    "        if step == 1 or step == 3:\n",
    "            zmin = 0\n",
    "            zmax= 100\n",
    "        images = batch['image']\n",
    "        origins = batch['original']#1, 1, 216, 220, 208\n",
    "        bgs = batch['bgmask']#1, 1, 216, 220, 208\n",
    "        info = nib.load(batch['image_meta_dict']['filename_or_obj'][0])\n",
    "        \n",
    "        low = np.zeros((images.shape[2], zmax-zmin+1, images.shape[4]))  \n",
    "        origin = np.zeros((images.shape[2], zmax-zmin+1, images.shape[4]))\n",
    "        bg = np.zeros((images.shape[2],zmax-zmin+1,images.shape[4]))\n",
    "        output = np.zeros((images.shape[2],zmax-zmin+1,images.shape[4]))\n",
    "        \n",
    "#         images = images.squeeze(0)\n",
    "#         origins = origins.squeeze(0)\n",
    "#         bgs = bgs.squeeze(0)\n",
    "#         break\n",
    "        for idx, i in enumerate(range(zmin, zmax)):\n",
    "            val_inputs =images[...,i,:].to(device)\n",
    "            bg_crop =bgs[...,i,:]\n",
    "            orig_crop =origins[...,i,:]\n",
    "\n",
    "#             val_outputs = sliding_window_inference(\n",
    "#                 val_inputs, [192, 192], 4, model, overlap=0.99,mode='gaussian')\n",
    "            val_outputs = model(val_inputs)\n",
    "            plt.figure(dpi = 200)\n",
    "            plt.subplot(131)\n",
    "            plt.imshow(orig_crop[0,0,...].detach().cpu().numpy(),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(132)\n",
    "            plt.imshow(val_inputs[0,0,...].detach().cpu().numpy(),cmap='gray')\n",
    "        \n",
    "            plt.axis('off')\n",
    "            plt.subplot(133)\n",
    "            plt.imshow(val_outputs[0,0,...].detach().cpu().numpy(),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "            \n",
    "#             print(val_outputs[0,0].shape)\n",
    "#             print(val_inputs[0,3].shape)\n",
    "#             print(new1[:,0,:].shape)\n",
    "            \n",
    "            output[:,idx,:] = val_outputs[0,0].detach().cpu().numpy()\n",
    "            origin[:,idx,:] = orig_crop[0,0]\n",
    "            bg[:,idx,:] = bg_crop[0,0]\n",
    "#             print(origins.shape,bgs.shape)\n",
    "            low[:,idx,:] = val_inputs[0,0].detach().cpu().numpy()\n",
    "#             print(bg_crop.shape)\n",
    "            \n",
    "#             plt.imshow(bg_crop[0,0])\n",
    "#             plt.show()\n",
    "            \n",
    "        \n",
    "    \n",
    "#         low_res_nifti = nib.Nifti1Image(low,info.affine,info.header)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_swinIR/low_T2/low_T2_{x}.nii.gz') \n",
    "        \n",
    "#         originals = nib.Nifti1Image(origin,info.affine,info.header)\n",
    "#         nib.save(originals,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_swinIR/T2/T2_{x}.nii.gz') \n",
    "        \n",
    "# #         bg = nib.Nifti1Image(bg,info.affine,info.header)\n",
    "# #         nib.save(bg,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_swinIR/bgmask/mask_{x}.nii.gz')    \n",
    "        \n",
    "#         output = nib.Nifti1Image(output,info.affine,info.header)\n",
    "#         nib.save(output,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_swinIR/syn_T2/syn_T2_{x}.nii.gz') \n",
    "#         x += 1\n",
    "#         low_res_nifti = nib.Nifti1Image(new1, af)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/result/0902/input_T2_{step}.nii.gz')   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84749b7",
   "metadata": {},
   "outputs": [],
   "source": [
    " low_res_nifti = nib.Nifti1Image(low,info.affine,info.header)\n",
    "        nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_unet/low/low_T2_{step}.nii.gz') \n",
    "        \n",
    "        originals = nib.Nifti1Image(origin,info.affine,info.header)\n",
    "        nib.save(originals,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_unet/T2/T2_{step}.nii.gz') \n",
    "        \n",
    "        bg = nib.Nifti1Image(bg,info.affine,info.header)\n",
    "        nib.save(bg,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_unet/bgmask/mask_{step}.nii.gz')    \n",
    "        \n",
    "        output = nib.Nifti1Image(output,info.affine,info.header)\n",
    "        nib.save(output,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/segmentation_unet/T2_syn/syn_T2_{step}.nii.gz')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245378be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "model.eval()\n",
    "psnr_value = []\n",
    "ssim_value = []\n",
    "\n",
    "\n",
    "zmin = 110\n",
    "zmax = 170\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        images = batch['image']#[1,1,216,220,220]\n",
    "        new1 = np.zeros((images.shape[2], images.shape[3], zmax-zmin+1))\n",
    "        new2 = np.zeros((images.shape[2], images.shape[3], zmax-zmin+1))\n",
    "        images = images.squeeze(1)\n",
    "#         break\n",
    "        for idx, i in enumerate(range(zmin, zmax)):\n",
    "            val_inputs =images[...,i-3:i+4].permute(0,3,1,2).to(device)\n",
    "            val_outputs = sliding_window_inference(\n",
    "                val_inputs, [192, 192], 4, model, overlap=0.95,mode='gaussian')\n",
    "#             val_outputs = model(val_inputs)\n",
    "            plt.figure(dpi = 200)\n",
    "            plt.subplot(121)\n",
    "            plt.imshow(np.rot90(val_inputs[0,3,...].detach().cpu().numpy()),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(122)\n",
    "            plt.imshow(np.rot90(val_outputs[0,0,...].detach().cpu().numpy()),cmap='gray')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            new1[:, :, idx] = val_outputs[0, 0].detach().cpu().numpy()\n",
    "            new2[:, :, idx] = val_inputs[0, 3].detach().cpu().numpy()\n",
    "            \n",
    "        \n",
    "        af = np.eye(4, 4)\n",
    "        \n",
    "#         low_res_nifti = nib.Nifti1Image(new2, af)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/result_0821/T2_{step}.nii.gz')   \n",
    "#         low_res_nifti = nib.Nifti1Image(new1, af)\n",
    "#         nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/result/result_0821/T2_{step}.nii.gz')   \n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d32b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIfTI 이미지 파일 경로 설정\n",
    "nifti_path = f'/nfs/hufsaims/kjh_shared/result/result_0821/T2_{step}.nii.gz'\n",
    "# NIfTI 이미지 불러오기\n",
    "nifti_image = nib.load(nifti_path)\n",
    "nifti_data = nifti_image.get_fdata()\n",
    "\n",
    "# 데이터의 최솟값과 최댓값을 이용하여 대비 조정\n",
    "min_value = np.min(nifti_data)\n",
    "max_value = np.max(nifti_data)\n",
    "\n",
    "# 대비 조정된 이미지 데이터\n",
    "contrast_normalized_data = (nifti_data - min_value) / (max_value - min_value)\n",
    "min_value = np.min(contrast_normalized_data)\n",
    "max_value = np.max(contrast_normalized_data)\n",
    "print(min_value,max_value)\n",
    "\n",
    "# # 대비 조정된 데이터를 다시 NIfTI 이미지로 저장\n",
    "# normalized_nifti_image = nib.Nifti1Image(contrast_normalized_data, nifti_image.affine)\n",
    "# nib.save(normalized_nifti_image, \"test.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6471a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIfTI 이미지 파일 경로 설정\n",
    "nifti_path = f'/nfs/hufsaims/kjh_shared/result/result_0821/T2_0_1.nii.gz'\n",
    "# NIfTI 이미지 불러오기\n",
    "nifti_image = nib.load(nifti_path)\n",
    "nifti_data = nifti_image.get_fdata()\n",
    "\n",
    "# 데이터의 최솟값과 최댓값을 이용하여 대비 조정\n",
    "min_value = np.min(nifti_data)\n",
    "max_value = np.max(nifti_data)\n",
    "\n",
    "# 대비 조정된 이미지 데이터\n",
    "contrast_normalized_data = (nifti_data - min_value) / (max_value - min_value)\n",
    "print(min_value,max_value)\n",
    "\n",
    "# # 대비 조정된 데이터를 다시 NIfTI 이미지로 저장\n",
    "# normalized_nifti_image = nib.Nifti1Image(contrast_normalized_data, nifti_image.affine)\n",
    "# nib.save(normalized_nifti_image, \"test.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = np.zeros((images.shape[2], images.shape[3], zmax-zmin+1))\n",
    "\n",
    "new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980df2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "images = images.squeeze(1)\n",
    "print(images.shape)\n",
    "val_inputs =images[...,i-3:i+4].permute(0,3,1,2).to(device)\n",
    "plt.imshow(np.rot90(val_inputs[0,6,...].detach().cpu().numpy()),cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fed254",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_nifti = nib.Nifti1Image(new1, af)\n",
    "nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/result/0823/T2_{step}_v2.nii.gz')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_nifti = nib.Nifti1Image(new2, af)\n",
    "nib.save(low_res_nifti,f'/nfs/hufsaims/kjh_shared/euna/data_bcp/result/0823/input_T2_{step}_v2.nii.gz') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3a27f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
